{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import optuna\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/train/final_labels.csv\")\n",
    "df = df[['body', 'level_1']].dropna()\n",
    "\n",
    "new_df = pd.read_csv(\"../datasets/train/SD_dataset_FINAL.csv\")\n",
    "comments_df = pd.read_excel(\"../datasets/train/sampled_comments.xlsx\")\n",
    "submissions_df = pd.read_excel(\"../datasets/train/sampled_submissions.xlsx\")\n",
    "\n",
    "combined_new = pd.concat([comments_df, submissions_df], ignore_index=True)\n",
    "combined_new['level_1'] = combined_new['label'].map({\n",
    "    'Neutral': 'Nonmisogynistic',\n",
    "    'Misogynistic': 'Misogynistic',\n",
    "    'Mentions Misogyny': 'Misogynistic'\n",
    "})\n",
    "\n",
    "combined_new = combined_new[['body', 'level_1']].dropna()\n",
    "\n",
    "new_df['level_1'] = new_df['level_1'].map({1: 'Misogynistic', 0: 'Nonmisogynistic'})\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "df = pd.concat([df, combined_new], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['level_1'])\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df[['body', 'label']])\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6637/6637 [00:02<00:00, 2901.49 examples/s]\n",
      "Map: 100%|██████████| 1660/1660 [00:00<00:00, 2848.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"MilaNLProc/bert-base-uncased-ear-misogyny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"best-misogyny-model\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"best-misogyny-model\")\n",
    "\n",
    "\n",
    "# 6. Tokenize the data\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"body\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "True\n",
      "NVIDIA RTX A2000 8GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # should show +cu118\n",
    "print(torch.cuda.is_available())  # should be True\n",
    "print(torch.cuda.get_device_name(0))  # should say \"NVIDIA RTX A2000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1660' max='1660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1660/1660 49:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.259894</td>\n",
       "      <td>0.896386</td>\n",
       "      <td>0.940731</td>\n",
       "      <td>0.898026</td>\n",
       "      <td>0.987699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.413300</td>\n",
       "      <td>0.367948</td>\n",
       "      <td>0.883133</td>\n",
       "      <td>0.928307</td>\n",
       "      <td>0.948640</td>\n",
       "      <td>0.908828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.476528</td>\n",
       "      <td>0.903012</td>\n",
       "      <td>0.942643</td>\n",
       "      <td>0.928421</td>\n",
       "      <td>0.957308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.514739</td>\n",
       "      <td>0.903012</td>\n",
       "      <td>0.942107</td>\n",
       "      <td>0.936383</td>\n",
       "      <td>0.947902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [104/104 00:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.47652769088745117, 'eval_accuracy': 0.903012048192771, 'eval_f1': 0.942643391521197, 'eval_precision': 0.9284210526315789, 'eval_recall': 0.9573082489146165, 'eval_runtime': 58.9378, 'eval_samples_per_second': 28.165, 'eval_steps_per_second': 1.765, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bert-ear-manual\\\\tokenizer_config.json',\n",
       " 'bert-ear-manual\\\\special_tokens_map.json',\n",
       " 'bert-ear-manual\\\\vocab.txt',\n",
       " 'bert-ear-manual\\\\added_tokens.json',\n",
       " 'bert-ear-manual\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def model_init():\n",
    "#     return AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions),\n",
    "        \"recall\": recall_score(labels, predictions),\n",
    "    }\n",
    "\n",
    "# def hp_space(trial):\n",
    "#     return {\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "#         \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
    "#         \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "#         \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "#     }\n",
    "\n",
    "# # 8. Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     metric_for_best_model=\"f1\"\n",
    "# )\n",
    "\n",
    "# 8. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "\n",
    "# def set_seed(seed=42):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# set_seed(42)\n",
    "\n",
    "# training_args = TrainingArguments(output_dir=\"./eval\", per_device_eval_batch_size=16)\n",
    "\n",
    "# 9. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_result)\n",
    "\n",
    "trainer.save_model(\"bert-ear-manual\")\n",
    "tokenizer.save_pretrained(\"bert-ear-manual\")\n",
    "\n",
    "# best_trial = trainer.hyperparameter_search(\n",
    "#     direction=\"maximize\",\n",
    "#     backend=\"optuna\",\n",
    "#     hp_space=hp_space,\n",
    "#     n_trials=10  # increase if you want a deeper search\n",
    "# )\n",
    "\n",
    "# print(\"Best trial:\")\n",
    "# print(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2490' max='2490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2490/2490 39:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.298205</td>\n",
       "      <td>0.890964</td>\n",
       "      <td>0.936558</td>\n",
       "      <td>0.908226</td>\n",
       "      <td>0.966715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.209200</td>\n",
       "      <td>0.394947</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.935159</td>\n",
       "      <td>0.931133</td>\n",
       "      <td>0.939219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.580403</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.940714</td>\n",
       "      <td>0.928773</td>\n",
       "      <td>0.952967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('best-misogyny-model\\\\tokenizer_config.json',\n",
       " 'best-misogyny-model\\\\special_tokens_map.json',\n",
       " 'best-misogyny-model\\\\vocab.txt',\n",
       " 'best-misogyny-model\\\\added_tokens.json',\n",
       " 'best-misogyny-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Extract best params\n",
    "best_params = best_trial.hyperparameters\n",
    "\n",
    "# Step 2: Update training args with them\n",
    "final_args = TrainingArguments(\n",
    "    output_dir=\"./best_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_params[\"num_train_epochs\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Step 3: Create a new Trainer with best settings\n",
    "final_trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=final_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Step 4: Train it!\n",
    "final_trainer.train()\n",
    "\n",
    "# Step 5 (optional): Save the model + tokenizer\n",
    "final_trainer.save_model(\"best-misogyny-model\")\n",
    "tokenizer.save_pretrained(\"best-misogyny-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.5804033875465393, 'eval_accuracy': 0.9, 'eval_f1': 0.9407142857142857, 'eval_precision': 0.9287729196050776, 'eval_recall': 0.9529667149059334, 'eval_runtime': 60.4833, 'eval_samples_per_second': 27.446, 'eval_steps_per_second': 3.439, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# # 11. Evaluate\n",
    "eval_result = final_trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_result)\n",
    "\n",
    "# # (Optional) Save model\n",
    "# trainer.save_model(\"misogyny-classifier\")\n",
    "# tokenizer.save_pretrained(\"misogyny-classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will run in batches under the hood and stay on GPU\n",
    "predictions = trainer.predict(test_dataset).predictions  # shape [n_examples, n_labels]\n",
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# now build your confusion matrix:\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(test_dataset[\"label\"], pred_labels)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Misogynistic\",\"Not Misogynistic\"])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "disp.plot(ax=ax, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"../images/bert-hyperparams-confusion.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1) Load the model & tokenizer from disk\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"best-misogyny-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"best-misogyny-model\")\n",
    "\n",
    "# 2) Set device for GPU if available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# 3) Load your custom dataset\n",
    "file_path = '../datasets/test/womenEngineers_comments_filtered.json'  # Update this path to your file\n",
    "output_csv_path = '../datasets/results/bert-hyperparams-comments.csv'\n",
    " \n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Prepare data for Trainer\n",
    "texts = []\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    body = data.get(\"body\", \"\").strip()  # Extracting 'body' field for comment text\n",
    "    texts.append(body)\n",
    "\n",
    "\n",
    "# Tokenize the texts using the tokenizer\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "# Convert inputs to a dataset\n",
    "inputs_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": inputs[\"input_ids\"],\n",
    "    \"attention_mask\": inputs[\"attention_mask\"]\n",
    "})\n",
    "\n",
    "# Create the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory to save results (not used for inference, but required)\n",
    "    per_device_eval_batch_size=16,  # Use 16 for batch size (or another value that fits your GPU memory)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,               # The fine-tuned model\n",
    "    args=training_args,        # TrainingArguments\n",
    "    tokenizer=tokenizer,       # The tokenizer used in the fine-tuning\n",
    ")\n",
    "\n",
    "# 4) Predict on the dataset using the trainer\n",
    "predictions = trainer.predict(inputs_dataset).predictions  # shape [n_examples, n_labels]\n",
    "pred_labels = predictions.argmax(axis=-1)  # Get the predicted labels (max probability)\n",
    "\n",
    "# 5) Save the results to a CSV file\n",
    "labeled_data = []\n",
    "for text, label in zip(texts, pred_labels):\n",
    "    sentiment = \"misogynistic\" if label == 0 else \"non-misogynistic\"\n",
    "    labeled_data.append({\"title\": text, \"sentiment\": sentiment})\n",
    "\n",
    "df = pd.DataFrame(labeled_data)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Saved labeled data to {output_csv_path}\")\n",
    "\n",
    "# 6) Plot the distribution of sentiment labels\n",
    "level_counts = {\"misogynistic\": sum(pred_labels == 0), \"non-misogynistic\": sum(pred_labels == 1)}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=list(level_counts.keys()), \n",
    "            y=list(level_counts.values()), \n",
    "            hue=list(level_counts.keys()), \n",
    "            palette=\"Blues\", \n",
    "            legend=False)\n",
    "\n",
    "plt.xlabel(\"Sentiment Level\")\n",
    "plt.ylabel(\"Number of Submissions\")\n",
    "plt.title(\"Distribution of Submissions Across Sentiment Levels (Bert EAR hyperparameters)\")\n",
    "plt.savefig(\"../images/hyperparameters-bert-submissions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(model_name)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Initialize sentiment analysis pipeline with truncation enabled\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m classifier = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msentiment-analysis\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Read initial datasets\u001b[39;00m\n\u001b[32m     22\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../datasets/train/final_labels.csv\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1180\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1178\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mprocessor\u001b[39m\u001b[33m\"\u001b[39m] = processor\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:85\u001b[39m, in \u001b[36mTextClassificationPipeline.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_model_type(\n\u001b[32m     88\u001b[39m         TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n\u001b[32m     89\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n\u001b[32m     91\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\transformers\\pipelines\\base.py:996\u001b[39m, in \u001b[36mPipeline.__init__\u001b[39m\u001b[34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    991\u001b[39m     \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.device != \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m    993\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.device, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device < \u001b[32m0\u001b[39m)\n\u001b[32m    994\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    995\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[38;5;66;03m# If the model can generate:\u001b[39;00m\n\u001b[32m    999\u001b[39m \u001b[38;5;66;03m# 1 - create a local generation config. This is done to avoid side-effects on the model as we apply local\u001b[39;00m\n\u001b[32m   1000\u001b[39m \u001b[38;5;66;03m# tweaks to the generation config.\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;66;03m# 2 - load the assistant model if it is passed.\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28mself\u001b[39m.assistant_model, \u001b[38;5;28mself\u001b[39m.assistant_tokenizer = load_assistant_model(\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33massistant_model\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33massistant_tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1004\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\transformers\\modeling_utils.py:3698\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3693\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3694\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3697\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3698\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1338\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMcmaho2\\programming\\FYP\\.venv311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1320\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1321\u001b[39m             device,\n\u001b[32m   1322\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1323\u001b[39m             non_blocking,\n\u001b[32m   1324\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1325\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "huggingface_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "# Load RoBERTa model and tokenizer\n",
    "# model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model_name = \"MilaNLProc/bert-base-uncased-ear-misogyny\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Initialize sentiment analysis pipeline with truncation enabled\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, truncation=True, max_length=512)\n",
    "\n",
    "# Read initial datasets\n",
    "df = pd.read_csv(\"../datasets/train/final_labels.csv\") \n",
    "df = df[['body', 'level_1']].dropna()\n",
    "\n",
    "new_df = pd.read_csv(\"../datasets/train/SD_dataset_FINAL.csv\")\n",
    "comments_df = pd.read_excel(\"../datasets/train/sampled_comments.xlsx\")\n",
    "submissions_df = pd.read_excel(\"../datasets/train/sampled_submissions.xlsx\")\n",
    "\n",
    "# Combine comments and submissions\n",
    "combined_new = pd.concat([comments_df, submissions_df], ignore_index=True)\n",
    "combined_new['level_1'] = combined_new['label'].map({\n",
    "    'Neutral': 'Nonmisogynistic',\n",
    "    'Misogynistic': 'Misogynistic',\n",
    "    'Mentions Misogyny': 'Misogynistic'\n",
    "})\n",
    "\n",
    "combined_new = combined_new[['body', 'level_1']].dropna()\n",
    "\n",
    "# Map new_df labels and combine datasets\n",
    "new_df['level_1'] = new_df['level_1'].map({1: 'Misogynistic', 0: 'Nonmisogynistic'})\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "df = pd.concat([df, combined_new], ignore_index=True)\n",
    "\n",
    "# Prepare lists for predictions\n",
    "pred_labels = []\n",
    "true_labels = df['level_1'].tolist()\n",
    "\n",
    "# List to store labeled data\n",
    "labeled_data = []\n",
    "\n",
    "# Process the dataset with a progress bar\n",
    "for idx, row in tqdm(df.iterrows(), desc=\"Classifying comments\", unit=\"comment\", total=len(df)):\n",
    "    input_text = row['body'].strip()\n",
    "\n",
    "    # Run sentiment classification\n",
    "    result = classifier(input_text)\n",
    "    sentiment_label = result[0]['label']  # \"LABEL_0\", \"LABEL_1\", \"LABEL_2\"\n",
    "\n",
    "    # Save the labeled data\n",
    "    labeled_data.append({\"body\": input_text, \"sentiment\": sentiment_label})\n",
    "    pred_labels.append(sentiment_label)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels, labels=[\"Nonmisogynistic\", \"Misogynistic\"])\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Misogynistic\", \"Nonmisogynistic\"])\n",
    "\n",
    "# Save confusion matrix plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "disp.plot(ax=ax, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"../images/bert-ear-confusion.png\")\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
